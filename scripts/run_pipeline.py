#!/usr/bin/env python
"""
ä¸»è¿è¡Œè„šæœ¬ - ç‰¹å¾å·¥ç¨‹æµæ°´çº¿
"""
import argparse
import yaml
import time
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'src'))

from preprocessor import TextProcessor
from vectorizer import FeatureVectorizer
from reducer import DimensionalityReducer
from visualization import visualize_results  # ä¼šåœ¨åé¢åˆ›å»º

def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def create_sample_data(n_samples: int = 1000) -> tuple:
    """
    åˆ›å»ºç¤ºä¾‹æ•°æ®
    å®é™…é¡¹ç›®ä¸­åº”è¯¥ä»æ–‡ä»¶åŠ è½½çœŸå®æ•°æ®
    """
    print(f"ç”Ÿæˆ {n_samples} ä¸ªç¤ºä¾‹æ–‡æœ¬...")
    
    # ç¤ºä¾‹æ–‡æœ¬ï¼ˆå®é™…åº”è¯¥ä»æ–‡ä»¶åŠ è½½ï¼‰
    sample_texts = [
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—çªç ´",
        "æœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒ",
        "ç‰¹å¾å·¥ç¨‹æ˜¯æœºå™¨å­¦ä¹ çš„å…³é”®æ­¥éª¤",
        "æ–‡æœ¬åˆ†ç±»éœ€è¦å¥½çš„ç‰¹å¾è¡¨ç¤ºæ–¹æ³•",
        "ä¸­æ–‡åˆ†è¯æ˜¯ä¸­æ–‡NLPçš„åŸºç¡€ä»»åŠ¡",
        "æƒ…æ„Ÿåˆ†æå¯ä»¥åˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
        "å‘½åå®ä½“è¯†åˆ«ç”¨äºè¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“",
        "æ–‡æœ¬æ‘˜è¦å¯ä»¥è‡ªåŠ¨ç”Ÿæˆæ–‡ç« æ‘˜è¦",
        "æœºå™¨ç¿»è¯‘å®ç°ä¸åŒè¯­è¨€ä¹‹é—´çš„è½¬æ¢"
    ]
    
    # æ‰©å±•æ ·æœ¬
    texts = []
    labels = []
    for i in range(n_samples):
        text = sample_texts[i % len(sample_texts)] + f" æ ·æœ¬ç¼–å· {i}"
        texts.append(text)
        labels.append(f"ç±»åˆ«_{i % 6}")  # 6ä¸ªç±»åˆ«
    
    return texts, labels

def run_pipeline(config: dict):
    """è¿è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹ç‰¹å¾å·¥ç¨‹æµæ°´çº¿")
    print("=" * 60)
    
    total_start_time = time.time()
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("\n1. åˆå§‹åŒ–ç»„ä»¶...")
    processor = TextProcessor(stopwords_path=config['data']['stopwords_path'])
    vectorizer = FeatureVectorizer(
        max_features=config['vectorization']['max_features'],
        max_df=config['vectorization']['max_df'],
        min_df=config['vectorization']['min_df'],
        norm=config['vectorization']['norm'],
        use_idf=config['vectorization']['use_idf'],
        sublinear_tf=config['vectorization']['sublinear_tf']
    )
    reducer = DimensionalityReducer(
        n_components=config['dimensionality_reduction']['n_components'],
        method=config['dimensionality_reduction']['method'],
        random_state=config['dimensionality_reduction']['random_state'],
        batch_size=config['dimensionality_reduction'].get('batch_size', 1000)
    )
    
    # 2. åŠ è½½æ•°æ®ï¼ˆè¿™é‡Œä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
    print("\n2. åŠ è½½æ•°æ®...")
    texts, labels = create_sample_data(config['data']['sample_size'])
    print(f"   åŠ è½½ {len(texts)} ä¸ªæ–‡æ¡£ï¼Œ{len(set(labels))} ä¸ªç±»åˆ«")
    
    # 3. æ–‡æœ¬é¢„å¤„ç†
    print("\n3. æ–‡æœ¬é¢„å¤„ç†...")
    preprocess_start = time.time()
    
    processed_texts = []
    for i, text in enumerate(texts):
        processed = processor.preprocess_to_text(
            text, 
            remove_stopwords=config['preprocessing']['remove_stopwords']
        )
        processed_texts.append(processed)
        
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 1000 == 0:
            print(f"   å·²å¤„ç† {i + 1}/{len(texts)} ä¸ªæ–‡æ¡£")
    
    preprocess_time = time.time() - preprocess_start
    print(f"   é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {preprocess_time:.2f}ç§’")
    
    # 4. ç‰¹å¾æå–
    print("\n4. ç‰¹å¾æå– (TF-IDF)...")
    vectorize_start = time.time()
    
    X, vocabulary = vectorizer.fit_transform(processed_texts)
    
    vectorize_time = time.time() - vectorize_start
    print(f"   ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {vectorize_time:.2f}ç§’")
    print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    
    # 5. é™ç»´
    print("\n5. ç»´åº¦å‹ç¼©...")
    reduce_start = time.time()
    
    X_reduced = reducer.fit_transform(X)
    
    reduce_time = time.time() - reduce_start
    print(f"   é™ç»´å®Œæˆï¼Œè€—æ—¶: {reduce_time:.2f}ç§’")
    print(f"   ç›®æ ‡ç»´åº¦: {X_reduced.shape[1]}")
    
    # 6. åˆ†æç»“æœ
    print("\n6. ç»“æœåˆ†æ...")
    
    # ç´¯è®¡è§£é‡Šæ–¹å·®
    cumulative_var = reducer.get_cumulative_variance()[-1]
    print(f"   ç´¯è®¡è§£é‡Šæ–¹å·®: {cumulative_var:.4f}")
    
    # è¾¾åˆ°90%æ–¹å·®æ‰€éœ€ç»´åº¦
    if reducer.explained_variance_ratio_ is not None:
        n_for_90 = reducer.get_variance_threshold(0.9)
        print(f"   è¾¾åˆ°90%æ–¹å·®æ‰€éœ€ç»´åº¦: {n_for_90}")
    
    # 7. å¯è§†åŒ–
    if config['output']['visualize']:
        print("\n7. ç”Ÿæˆå¯è§†åŒ–...")
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = config['output']['output_dir']
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualize_results(
                X_reduced=X_reduced,
                labels=labels,
                reducer=reducer,
                output_dir=output_dir,
                config=config['output']['visualization']
            )
            print(f"   å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_dir}")
        except Exception as e:
            print(f"   å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    # 8. ä¿å­˜ç»“æœ
    if config['output']['save_features']:
        print("\n8. ä¿å­˜ç»“æœ...")
        output_dir = config['output']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜é™ç»´åçš„ç‰¹å¾
        if 'npy' in config['output']['formats']:
            np.save(os.path.join(output_dir, 'features_reduced.npy'), X_reduced)
            print(f"   ä¿å­˜ä¸ºNumPyæ ¼å¼: features_reduced.npy")
        
        # ä¿å­˜æ ‡ç­¾
        np.save(os.path.join(output_dir, 'labels.npy'), np.array(labels))
        
        # ä¿å­˜è¯æ±‡è¡¨
        if vocabulary is not None:
            np.save(os.path.join(output_dir, 'vocabulary.npy'), vocabulary)
        
        # ä¿å­˜æ¨¡å‹
        vectorizer.save(os.path.join(output_dir, 'vectorizer.pkl'))
        reducer.save(os.path.join(output_dir, 'reducer.pkl'))
    
    # 9. ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - total_start_time
    print("\n" + "=" * 60)
    print("âœ… æµæ°´çº¿å®Œæˆ!")
    print("=" * 60)
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"åŸå§‹ç»´åº¦: {X.shape[1]} â†’ é™ç»´å: {X_reduced.shape[1]}")
    print(f"å‹ç¼©æ¯”ä¾‹: {(1 - X_reduced.shape[1] / X.shape[1]) * 100:.1f}%")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {cumulative_var:.4f}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ–‡æœ¬ç‰¹å¾å·¥ç¨‹æµæ°´çº¿')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/config.yaml)')
    parser.add_argument('--sample_size', type=int, default=None,
                       help='æ ·æœ¬æ•°é‡ (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("è¯·ç¡®ä¿ config/config.yaml æ–‡ä»¶å­˜åœ¨")
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è¦†ç›–æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼‰
    if args.sample_size:
        config['data']['sample_size'] = args.sample_size
    
    # è¿è¡Œæµæ°´çº¿
    try:
        run_pipeline(config)
    except Exception as e:
        print(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # éœ€è¦numpyï¼Œä½†åªåœ¨è¿è¡Œæ—¶å¯¼å…¥
    import numpy as np
    main()
